{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3eac6c",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Project: Customer Segmentation Analysis\n",
    "\n",
    "## GitHub\n",
    "GitHub URL: https://github.com/xllcheryl/Unsupervised-Algorithms-in-Machine-Learning-Final-Project\n",
    "\n",
    "## 1. Problem Description and Data Collection\n",
    "\n",
    "### 1.1 Project Overview\n",
    "This project focuses on customer segmentation using unsupervised learning techniques. The goal is to identify distinct groups of customers based on their purchasing behavior and demographic characteristics. This segmentation can help businesses develop targeted marketing strategies and personalized customer experiences.\n",
    "\n",
    "### 1.2 Dataset Description\n",
    "The dataset used is the \"Mall Customer Segmentation Data\" from Kaggle, which contains basic information about mall customers. This is a publicly available dataset commonly used for clustering exercises.\n",
    "\n",
    "**Data Provenance:**\n",
    "- Source: https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n",
    "- Collection Method: The data was likely collected through mall membership cards and customer surveys\n",
    "- Contains: 200 records with 5 customer attributes\n",
    "\n",
    "### 1.3 Business Problem\n",
    "The primary objective is to identify distinct customer segments that exhibit similar behaviors and characteristics. By understanding these segments, businesses can:\n",
    "- Develop targeted marketing campaigns\n",
    "- Optimize product offerings for different customer groups\n",
    "- Improve customer retention through personalized experiences\n",
    "- Allocate resources more effectively based on customer value\n",
    "\n",
    "### 1.4 Analytical Approach\n",
    "\n",
    "#### 1.4.1. Exploratory Data Analysis (EDA)\n",
    "- Distribution analysis of all features\n",
    "- Correlation analysis between variables\n",
    "- Identification of patterns and relationships in the data\n",
    "- Outlier detection and treatment\n",
    "\n",
    "#### 1.4.2. Feature Engineering\n",
    "- Encoding categorical variables\n",
    "- Feature scaling and normalization\n",
    "- Dimensionality reduction using PCA\n",
    "\n",
    "#### 1.4.3. Model Development\n",
    "Multiple clustering algorithms will be implemented and compared:\n",
    "\n",
    "- **K-Means Clustering**: Partitioning method that separates data into k clusters\n",
    "- **Gaussian Mixture Models (GMM)**: Probabilistic approach that assumes data points are generated from a mixture of Gaussian distributions\n",
    "- **Hierarchical Clustering**: Builds a hierarchy of clusters either agglomeratively or divisively\n",
    "- **DBSCAN**: density-based, noise-aware\n",
    "\n",
    "#### 1.4.4. Evaluation Metrics\n",
    "Multiple internal validation metrics will be used:\n",
    "- **Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters\n",
    "- **Calinski-Harabasz Index**: Ratio of between-clusters dispersion to within-cluster dispersion\n",
    "- **Davies-Bouldin Index**: Average similarity measure of each cluster with its most similar cluster\n",
    "\n",
    "### Expected Outcomes\n",
    "1. Identification of distinct customer segments with unique characteristics\n",
    "2. Comparison of different clustering algorithms\n",
    "3. Actionable insights for targeted marketing strategies\n",
    "4. Understanding of customer behavior patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4784b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0fe17",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a46d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Mall_Customers.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd508603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad1a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51aa1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate entries\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7657afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b820c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for categorical columns\n",
    "print(\"Unique values in Gender:\", df['Gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39b219",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1402b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Age distribution\n",
    "sns.histplot(df['Age'], bins=20, kde=True, ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Annual Income distribution\n",
    "sns.histplot(df['Annual Income (k$)'], bins=20, kde=True, ax=axes[0, 1], color='lightcoral')\n",
    "axes[0, 1].set_title('Annual Income Distribution')\n",
    "axes[0, 1].set_xlabel('Annual Income (k$)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Spending Score distribution\n",
    "sns.histplot(df['Spending Score (1-100)'], bins=20, kde=True, ax=axes[1, 0], color='lightgreen')\n",
    "axes[1, 0].set_title('Spending Score Distribution')\n",
    "axes[1, 0].set_xlabel('Spending Score (1-100)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = df['Gender'].value_counts()\n",
    "axes[1, 1].bar(gender_counts.index, gender_counts.values, color=['lightpink', 'lightblue'])\n",
    "axes[1, 1].set_title('Gender Distribution')\n",
    "axes[1, 1].set_xlabel('Gender')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "for i, v in enumerate(gender_counts.values):\n",
    "    axes[1, 1].text(i, v + 2, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for numerical features to identify outliers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "sns.boxplot(y=df['Age'], ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Age Boxplot')\n",
    "\n",
    "sns.boxplot(y=df['Annual Income (k$)'], ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('Annual Income Boxplot')\n",
    "\n",
    "sns.boxplot(y=df['Spending Score (1-100)'], ax=axes[2], color='lightgreen')\n",
    "axes[2].set_title('Spending Score Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfeded",
   "metadata": {},
   "source": [
    "### 3.2 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd92b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships between numerical features\n",
    "sns.pairplot(df, hue='Gender', diag_kind='kde', palette='Set2')\n",
    "plt.suptitle('Pairplot of Numerical Features by Gender', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e6a8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "numeric_df = df.select_dtypes(include=[np.number]).drop('CustomerID', axis=1)\n",
    "correlation_matrix = numeric_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c64d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between Age and Spending Score by Gender\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age', y='Spending Score (1-100)', hue='Gender', data=df, palette='Set2', s=80)\n",
    "plt.title('Age vs Spending Score by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6854ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between Annual Income and Spending Score by Gender\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Gender', data=df, palette='Set2', s=80)\n",
    "plt.title('Annual Income vs Spending Score by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49453f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots to show distribution of spending by gender and age groups\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 30, 40, 50, 100], labels=['<30', '30-40', '40-50', '50+'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x='AgeGroup', y='Spending Score (1-100)', hue='Gender', data=df, palette='Set2', split=True)\n",
    "plt.title('Spending Score Distribution by Age Group and Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2e6f1",
   "metadata": {},
   "source": [
    "### 3.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b47ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color by gender\n",
    "colors = {'Male': 'blue', 'Female': 'red'}\n",
    "ax.scatter(df['Age'], df['Annual Income (k$)'], df['Spending Score (1-100)'], \n",
    "           c=df['Gender'].map(colors), s=60, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Annual Income (k$)')\n",
    "ax.set_zlabel('Spending Score (1-100)')\n",
    "ax.set_title('3D View of Customer Data')\n",
    "\n",
    "# Create a legend\n",
    "import matplotlib.patches as mpatches\n",
    "legend_elements = [mpatches.Patch(color='blue', label='Male'),\n",
    "                   mpatches.Patch(color='red', label='Female')]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1a60b",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84fe0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encode categorical variable (Gender)\n",
    "df_processed['Gender'] = df_processed['Gender'].map({'Female': 0, 'Male': 1})\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)', 'Gender']\n",
    "X = df_processed[features]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Scaled data shape:\", X_scaled.shape)\n",
    "print(\"First 5 rows of scaled data:\")\n",
    "print(X_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b64af",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b7b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df = pd.concat([pca_df, df['Gender']], axis=1)\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Gender', data=pca_df, palette='Set2', s=80)\n",
    "plt.title('PCA: PC1 vs PC2 colored by Gender')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.legend(title='Gender')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6f3375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 3 components for 3D visualization\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color by gender\n",
    "colors = df_processed['Gender'].map({0: 'red', 1: 'blue'})\n",
    "ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], \n",
    "           c=colors, s=60, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%} variance)')\n",
    "ax.set_title('3D PCA Visualization')\n",
    "\n",
    "# Create a legend\n",
    "legend_elements = [mpatches.Patch(color='red', label='Female'),\n",
    "                   mpatches.Patch(color='blue', label='Male')]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance ratio: {pca_3d.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca_3d.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba27504",
   "metadata": {},
   "source": [
    "## 6. Determining Optimal Number of Clusters\n",
    "\n",
    "### 6.1 Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5866e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal k\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker='o', linestyle='--', color='teal')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc90681",
   "metadata": {},
   "source": [
    "### 6.2 Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85fb7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='--', color='purple')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal k')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1d49e",
   "metadata": {},
   "source": [
    "### 6.3 Gap Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e7c16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gap statistic (simplified approach)\n",
    "def calculate_gap_statistic(data, k_max=10, n_bootstraps=10):\n",
    "    \"\"\"\n",
    "    Calculate gap statistic for k-means clustering\n",
    "    \"\"\"\n",
    "    # Reference dispersion using uniform random data\n",
    "    min_vals = data.min(axis=0)\n",
    "    max_vals = data.max(axis=0)\n",
    "    \n",
    "    gaps = []\n",
    "    sks = []\n",
    "    \n",
    "    for k in range(1, k_max + 1):\n",
    "        # Cluster on real data\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        wk = np.log(kmeans.inertia_)\n",
    "        \n",
    "        # Cluster on reference data\n",
    "        wk_refs = []\n",
    "        for _ in range(n_bootstraps):\n",
    "            ref_data = np.random.uniform(min_vals, max_vals, size=data.shape)\n",
    "            kmeans_ref = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans_ref.fit(ref_data)\n",
    "            wk_refs.append(np.log(kmeans_ref.inertia_))\n",
    "        \n",
    "        # Calculate gap statistic\n",
    "        gap = np.mean(wk_refs) - wk\n",
    "        gaps.append(gap)\n",
    "        \n",
    "        # Calculate standard error\n",
    "        sk = np.sqrt(np.var(wk_refs)) * np.sqrt(1 + 1 / n_bootstraps)\n",
    "        sks.append(sk)\n",
    "    \n",
    "    return gaps, sks\n",
    "\n",
    "# Calculate gap statistics\n",
    "gaps, sks = calculate_gap_statistic(X_scaled)\n",
    "\n",
    "# Plot gap statistic\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_range = range(1, 11)\n",
    "plt.plot(k_range, gaps, marker='o', linestyle='--', color='orange')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Gap Statistic')\n",
    "plt.title('Gap Statistic for Optimal k')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1087d",
   "metadata": {},
   "source": [
    "### 6.4 Dendrogram for Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63091467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram for hierarchical clustering\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))\n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean Distances')\n",
    "plt.axhline(y=10, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1a845",
   "metadata": {},
   "source": [
    "## 7. Model Building and Training\n",
    "\n",
    "### 7.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a89f4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal k\n",
    "optimal_k = 5\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df['KMeans_Cluster'] = kmeans_labels\n",
    "\n",
    "# Evaluate K-Means\n",
    "kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
    "kmeans_ch_score = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "kmeans_db_score = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.3f}\")\n",
    "print(f\"K-Means Calinski-Harabasz Score: {kmeans_ch_score:.3f}\")\n",
    "print(f\"K-Means Davies-Bouldin Score: {kmeans_db_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09277243",
   "metadata": {},
   "source": [
    "### 7.2 Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1633e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df['GMM_Cluster'] = gmm_labels\n",
    "\n",
    "# Evaluate GMM\n",
    "gmm_silhouette = silhouette_score(X_scaled, gmm_labels)\n",
    "gmm_ch_score = calinski_harabasz_score(X_scaled, gmm_labels)\n",
    "gmm_db_score = davies_bouldin_score(X_scaled, gmm_labels)\n",
    "\n",
    "print(f\"GMM Silhouette Score: {gmm_silhouette:.3f}\")\n",
    "print(f\"GMM Calinski-Harabasz Score: {gmm_ch_score:.3f}\")\n",
    "print(f\"GMM Davies-Bouldin Score: {gmm_db_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300028f",
   "metadata": {},
   "source": [
    "### 7.3 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3baa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df['Hierarchical_Cluster'] = agg_labels\n",
    "\n",
    "# Evaluate Hierarchical Clustering\n",
    "agg_silhouette = silhouette_score(X_scaled, agg_labels)\n",
    "agg_ch_score = calinski_harabasz_score(X_scaled, agg_labels)\n",
    "agg_db_score = davies_bouldin_score(X_scaled, agg_labels)\n",
    "\n",
    "print(f\"Hierarchical Clustering Silhouette Score: {agg_silhouette:.3f}\")\n",
    "print(f\"Hierarchical Clustering Calinski-Harabasz Score: {agg_ch_score:.3f}\")\n",
    "print(f\"Hierarchical Clustering Davies-Bouldin Score: {agg_db_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328b0bb",
   "metadata": {},
   "source": [
    "### 7.4 DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffccb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Check number of clusters found (excluding noise points labeled as -1)\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"DBSCAN found {n_clusters} clusters and {n_noise} noise points.\")\n",
    "\n",
    "# Only evaluate if meaningful clusters were found\n",
    "if n_clusters > 1:\n",
    "    dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels)\n",
    "    dbscan_ch_score = calinski_harabasz_score(X_scaled, dbscan_labels)\n",
    "    dbscan_db_score = davies_bouldin_score(X_scaled, dbscan_labels)\n",
    "    \n",
    "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.3f}\")\n",
    "    print(f\"DBSCAN Calinski-Harabasz Score: {dbscan_ch_score:.3f}\")\n",
    "    print(f\"DBSCAN Davies-Bouldin Score: {dbscan_db_score:.3f}\")\n",
    "    \n",
    "    # Add to dataframe if meaningful\n",
    "    df['DBSCAN_Cluster'] = dbscan_labels\n",
    "else:\n",
    "    print(\"DBSCAN did not find meaningful clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8a777",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis\n",
    "\n",
    "### 8.1 Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbad0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using the first two principal components\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# K-Means clusters\n",
    "scatter1 = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', s=80)\n",
    "axes[0, 0].set_title(f'K-Means Clustering (k={optimal_k})')\n",
    "axes[0, 0].set_xlabel('PC1')\n",
    "axes[0, 0].set_ylabel('PC2')\n",
    "plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "\n",
    "# GMM clusters\n",
    "scatter2 = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=gmm_labels, cmap='viridis', s=80)\n",
    "axes[0, 1].set_title(f'GMM Clustering (k={optimal_k})')\n",
    "axes[0, 1].set_xlabel('PC1')\n",
    "axes[0, 1].set_ylabel('PC2')\n",
    "plt.colorbar(scatter2, ax=axes[0, 1])\n",
    "\n",
    "# Hierarchical clusters\n",
    "scatter3 = axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, cmap='viridis', s=80)\n",
    "axes[1, 0].set_title(f'Hierarchical Clustering (k={optimal_k})')\n",
    "axes[1, 0].set_xlabel('PC1')\n",
    "axes[1, 0].set_ylabel('PC2')\n",
    "plt.colorbar(scatter3, ax=axes[1, 0])\n",
    "\n",
    "# DBSCAN clusters if available\n",
    "if 'DBSCAN_Cluster' in df.columns:\n",
    "    scatter4 = axes[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='viridis', s=80)\n",
    "    axes[1, 1].set_title(f'DBSCAN Clustering')\n",
    "    axes[1, 1].set_xlabel('PC1')\n",
    "    axes[1, 1].set_ylabel('PC2')\n",
    "    plt.colorbar(scatter4, ax=axes[1, 1])\n",
    "else:\n",
    "    axes[1, 1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "924492e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization of K-Means clusters\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Using original features for interpretability\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(df['Age'], df['Annual Income (k$)'], df['Spending Score (1-100)'], \n",
    "                    c=kmeans_labels, cmap='viridis', s=60, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Annual Income (k$)')\n",
    "ax.set_zlabel('Spending Score (1-100)')\n",
    "ax.set_title('3D View of K-Means Clusters')\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65cf8c",
   "metadata": {},
   "source": [
    "### 8.2 Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91cbf334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics (using K-Means results)\n",
    "cluster_profile = df.groupby('KMeans_Cluster').agg({\n",
    "    'Age': ['mean', 'std'],\n",
    "    'Annual Income (k$)': ['mean', 'std'],\n",
    "    'Spending Score (1-100)': ['mean', 'std'],\n",
    "    'Gender': lambda x: (x == 'Female').sum() / len(x) * 100  # Percentage of females\n",
    "}).round(2)\n",
    "\n",
    "cluster_profile.columns = ['Avg Age', 'Age Std', 'Avg Income', 'Income Std', \n",
    "                          'Avg Spending', 'Spending Std', 'Female %']\n",
    "print(\"Cluster Profile (K-Means):\")\n",
    "cluster_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cf796ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Age distribution by cluster\n",
    "sns.boxplot(x='KMeans_Cluster', y='Age', data=df, ax=axes[0, 0], palette='Set2')\n",
    "axes[0, 0].set_title('Age Distribution by Cluster')\n",
    "axes[0, 0].set_xlabel('Cluster')\n",
    "axes[0, 0].set_ylabel('Age')\n",
    "\n",
    "# Income distribution by cluster\n",
    "sns.boxplot(x='KMeans_Cluster', y='Annual Income (k$)', data=df, ax=axes[0, 1], palette='Set2')\n",
    "axes[0, 1].set_title('Annual Income Distribution by Cluster')\n",
    "axes[0, 1].set_xlabel('Cluster')\n",
    "axes[0, 1].set_ylabel('Annual Income (k$)')\n",
    "\n",
    "# Spending score distribution by cluster\n",
    "sns.boxplot(x='KMeans_Cluster', y='Spending Score (1-100)', data=df, ax=axes[1, 0], palette='Set2')\n",
    "axes[1, 0].set_title('Spending Score Distribution by Cluster')\n",
    "axes[1, 0].set_xlabel('Cluster')\n",
    "axes[1, 0].set_ylabel('Spending Score (1-100)')\n",
    "\n",
    "# Gender distribution by cluster\n",
    "gender_cluster = pd.crosstab(df['KMeans_Cluster'], df['Gender'], normalize='index') * 100\n",
    "gender_cluster.plot(kind='bar', ax=axes[1, 1], color=['lightpink', 'lightblue'])\n",
    "axes[1, 1].set_title('Gender Distribution by Cluster')\n",
    "axes[1, 1].set_ylabel('Percentage')\n",
    "axes[1, 1].set_xlabel('Cluster')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "axes[1, 1].legend(title='Gender')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78e875ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot colored by cluster\n",
    "sns.pairplot(df, hue='KMeans_Cluster', vars=['Age', 'Annual Income (k$)', 'Spending Score (1-100)'], \n",
    "             palette='Set2', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Features Colored by K-Means Cluster', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30dea6",
   "metadata": {},
   "source": [
    "### 8.3 Comparison of Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dda30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare algorithm performance\n",
    "algorithms = ['K-Means', 'GMM', 'Hierarchical']\n",
    "silhouette_scores = [kmeans_silhouette, gmm_silhouette, agg_silhouette]\n",
    "ch_scores = [kmeans_ch_score, gmm_ch_score, agg_ch_score]\n",
    "db_scores = [kmeans_db_score, gmm_db_score, agg_db_score]\n",
    "\n",
    "# Add DBSCAN if available\n",
    "if 'DBSCAN_Cluster' in df.columns:\n",
    "    algorithms.append('DBSCAN')\n",
    "    silhouette_scores.append(dbscan_silhouette)\n",
    "    ch_scores.append(dbscan_ch_score)\n",
    "    db_scores.append(dbscan_db_score)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Algorithm': algorithms,\n",
    "    'Silhouette Score': silhouette_scores,\n",
    "    'Calinski-Harabasz Score': ch_scores,\n",
    "    'Davies-Bouldin Score': db_scores\n",
    "}).round(3)\n",
    "\n",
    "print(\"Algorithm Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15a1ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize algorithm comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0].bar(algorithms, silhouette_scores, color='lightseagreen')\n",
    "axes[0].set_title('Silhouette Score Comparison')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Calinski-Harabasz Score\n",
    "axes[1].bar(algorithms, ch_scores, color='lightcoral')\n",
    "axes[1].set_title('Calinski-Harabasz Score Comparison')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Davies-Bouldin Score (lower is better)\n",
    "axes[2].bar(algorithms, db_scores, color='lightsteelblue')\n",
    "axes[2].set_title('Davies-Bouldin Score Comparison')\n",
    "axes[2].set_ylabel('Score (Lower is Better)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96414074",
   "metadata": {},
   "source": [
    "## 9. Discussion and Conclusion\n",
    "\n",
    "### 9.1 Interpretation of Customer Segments\n",
    "\n",
    "Based on the K-Means clustering results (which performed best), we can interpret the customer segments as follows:\n",
    "\n",
    "1. **Cluster 0 (High Income, High Spending)**: Younger to middle-aged customers with high income and high spending score. This represents the premium segment that should be targeted with luxury products and exclusive offers.\n",
    "\n",
    "2. **Cluster 1 (High Income, Low Spending)**: Middle-aged customers with high income but low spending score. This group may be focused on saving or investing rather than discretionary spending. They might respond better to value-based messaging.\n",
    "\n",
    "3. **Cluster 2 (Moderate Income, Moderate Spending)**: Older female customers with moderate income and spending. This segment may represent practical shoppers who make considered purchases.\n",
    "\n",
    "4. **Cluster 3 (Low Income, High Spending)**: Young customers with lower income but high spending score. This group may be more fashion-conscious or impulsive spenders despite limited resources. Discounts and promotional offers may appeal to this segment.\n",
    "\n",
    "5. **Cluster 4 (Moderate Income, Low Spending)**: Older male customers with moderate income but low spending. This group may be conservative in their spending habits and value functionality over fashion.\n",
    "\n",
    "### 9.2 Algorithm Performance Analysis\n",
    "\n",
    "The comparison of clustering algorithms reveals important insights:\n",
    "\n",
    "1. **K-Means** performed best across all metrics with the highest Silhouette Score, highest Calinski-Harabasz Score, and lowest Davies-Bouldin Score. This suggests that the data has spherical clusters that K-Means can effectively capture.\n",
    "\n",
    "2. **Hierarchical Clustering** performed respectably but slightly worse than K-Means, indicating that while the hierarchical structure captures some patterns, the spherical assumption of K-Means better fits this dataset.\n",
    "\n",
    "3. **GMM** showed the weakest performance, suggesting that the assumption of Gaussian-distributed clusters may not perfectly align with the data structure.\n",
    "\n",
    "4. **DBSCAN** either didn't find meaningful clusters or performed poorly, indicating that the data doesn't have clear density-based clusters.\n",
    "\n",
    "The moderate Silhouette Scores across all algorithms indicate that while the clusters are meaningful, there is some overlap between them, which is expected in real-world customer data.\n",
    "\n",
    "### 9.3 Business Implications and Recommendations\n",
    "\n",
    "1. **Personalized Marketing Strategies**:\n",
    "   - **Cluster 0**: Premium products, exclusive events, loyalty programs\n",
    "   - **Cluster 1**: Value-based messaging, investment-oriented products, financial planning services\n",
    "   - **Cluster 2**: Practical products, quality-focused messaging, reliability emphasis\n",
    "   - **Cluster 3**: Trendy items, discounts, payment plans, social media marketing\n",
    "   - **Cluster 4**: Functional products, durability messaging, practical benefits emphasis\n",
    "\n",
    "2. **Product Placement and Assortment**:\n",
    "   - Position luxury items in areas frequented by Cluster 0 customers\n",
    "   - Create value sections for Cluster 1 customers\n",
    "   - Ensure practical and reliable products are easily accessible for Cluster 2 and 4 customers\n",
    "   - Feature trendy and promotional items prominently for Cluster 3 customers\n",
    "\n",
    "3. **Customer Retention Strategies**:\n",
    "   - Develop loyalty programs tailored to each segment's preferences\n",
    "   - Create personalized communication based on cluster characteristics\n",
    "   - Implement targeted retention offers for at-risk segments\n",
    "\n",
    "4. **Resource Allocation**:\n",
    "   - Focus marketing resources on high-potential segments (Clusters 0 and 3)\n",
    "   - Develop cost-effective strategies for lower-spending segments\n",
    "   - Prioritize customer service for high-value segments\n",
    "\n",
    "### 9.4 Limitations and Future Work\n",
    "\n",
    "**Limitations:**\n",
    "1. The dataset is relatively small (200 samples), which may affect the generalizability of the findings.\n",
    "2. The analysis only considered available features; additional customer data (purchase history, preferences) could provide deeper insights.\n",
    "3. The clustering results, while mathematically sound, should be validated with domain expertise.\n",
    "4. The optimal number of clusters was determined mathematically, but business considerations might suggest a different number.\n",
    "\n",
    "**Future Work:**\n",
    "1. Incorporate additional data sources such as purchase history, online behavior, and customer feedback.\n",
    "2. Experiment with deep learning-based clustering approaches like autoencoders.\n",
    "3. Implement dynamic clustering that updates segments based on real-time customer behavior.\n",
    "4. Conduct A/B testing to validate the effectiveness of cluster-based marketing strategies.\n",
    "5. Explore ensemble clustering methods to combine strengths of different algorithms.\n",
    "6. Develop a customer segmentation dashboard for business users.\n",
    "\n",
    "### 9.5 Conclusion\n",
    "\n",
    "This project successfully demonstrated the application of unsupervised learning techniques for customer segmentation. By employing K-Means, GMM, and hierarchical clustering algorithms, we identified five distinct customer segments with unique demographic and behavioral characteristics. \n",
    "\n",
    "K-Means emerged as the most effective algorithm for this dataset, achieving the best performance across all evaluation metrics. The resulting segments provide actionable insights for targeted marketing, product placement, and customer relationship management strategies.\n",
    "\n",
    "The methodology and findings can serve as a template for similar customer segmentation problems in retail and e-commerce environments. The relatively moderate performance scores across all algorithms also highlight the complexity of real-world customer data and the importance of continuous refinement and validation of clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e42c3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "df.to_csv('customer_segmentation_results.csv', index=False)\n",
    "print(\"Results saved to 'customer_segmentation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cc4f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the model for future use\n",
    "import joblib\n",
    "\n",
    "# Save the scaler and K-Means model\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
    "print(\"Model artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcc402",
   "metadata": {},
   "source": [
    "## GitHub\n",
    "GitHub URL: https://github.com/xllcheryl/Unsupervised-Algorithms-in-Machine-Learning-Final-Project"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
